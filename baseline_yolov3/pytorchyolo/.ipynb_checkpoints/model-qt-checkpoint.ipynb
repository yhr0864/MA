{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load models.py\n",
    "from __future__ import division\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from utils.parse_config import parse_model_config\n",
    "from utils.utils import weights_init_normal\n",
    "from utils import torch_utils\n",
    "\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "from quant_dorefa import QuanConv as Conv_q\n",
    "\n",
    "import torch.quantization.quantize_fx as quantize_fx\n",
    "import copy\n",
    "from torch.fx import symbolic_trace\n",
    "\n",
    "\n",
    "def create_modules(module_defs):\n",
    "    \"\"\"\n",
    "    Constructs module list of layer blocks from module configuration in module_defs\n",
    "    module_list: nn.ModuleList(nn.Sequential(nn.Conv2d(...),nn.LeakyReLU(0.1)), ...\n",
    "                               nn.Sequential(nn.MaxPool2d(...)), ...\n",
    "                               nn.Sequential(yolo_layer)\n",
    "                               )\n",
    "    \"\"\"\n",
    "    hyperparams = module_defs.pop(0)\n",
    "    hyperparams.update({\n",
    "        'batch': int(hyperparams['batch']),\n",
    "        'subdivisions': int(hyperparams['subdivisions']),\n",
    "        'width': int(hyperparams['width']),\n",
    "        'height': int(hyperparams['height']),\n",
    "        'channels': int(hyperparams['channels']),\n",
    "        'optimizer': hyperparams.get('optimizer'),\n",
    "        'momentum': float(hyperparams['momentum']),\n",
    "        'decay': float(hyperparams['decay']),\n",
    "        'learning_rate': float(hyperparams['learning_rate']),\n",
    "        'burn_in': int(hyperparams['burn_in']),\n",
    "        'max_batches': int(hyperparams['max_batches']),\n",
    "        'policy': hyperparams['policy'],\n",
    "        'lr_steps': list(zip(map(int,   hyperparams[\"steps\"].split(\",\")),\n",
    "                             map(float, hyperparams[\"scales\"].split(\",\"))))\n",
    "    })\n",
    "    assert hyperparams[\"height\"] == hyperparams[\"width\"], \\\n",
    "        \"Height and width should be equal! Non square images are padded with zeros.\"\n",
    "    output_filters = [hyperparams[\"channels\"]]\n",
    "    module_list = nn.ModuleList()\n",
    "    for module_i, module_def in enumerate(module_defs):\n",
    "        modules = nn.Sequential()\n",
    "               \n",
    "        if module_def[\"type\"] == \"convolutional\":\n",
    "            bn = int(module_def[\"batch_normalize\"])\n",
    "            filters = int(module_def[\"filters\"])\n",
    "            kernel_size = int(module_def[\"size\"])\n",
    "            pad = (kernel_size - 1) // 2\n",
    "            modules.add_module(\n",
    "                \"conv\",\n",
    "                nn.Conv2d(\n",
    "                    in_channels=output_filters[-1],\n",
    "                    out_channels=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=int(module_def[\"stride\"]),\n",
    "                    padding=pad,\n",
    "                    bias=not bn,\n",
    "                ),\n",
    "            )\n",
    "            if bn:\n",
    "                modules.add_module(\"batch_norm\",\n",
    "                                   nn.BatchNorm2d(filters, momentum=0.1, eps=1e-5))\n",
    "            if module_def[\"activation\"] == \"leaky\":\n",
    "                modules.add_module(f\"leaky_{module_i}\", nn.LeakyReLU(0.1))\n",
    "            if module_def[\"activation\"] == \"mish\":\n",
    "                modules.add_module(f\"mish_{module_i}\", Mish())\n",
    "\n",
    "        elif module_def[\"type\"] == \"maxpool\":\n",
    "            kernel_size = int(module_def[\"size\"])\n",
    "            stride = int(module_def[\"stride\"])\n",
    "            if kernel_size == 2 and stride == 1:\n",
    "                modules.add_module(f\"_debug_padding_{module_i}\", nn.ZeroPad2d((0, 1, 0, 1)))\n",
    "            maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride,\n",
    "                                   padding=int((kernel_size - 1) // 2))\n",
    "            modules.add_module(f\"maxpool_{module_i}\", maxpool)\n",
    "\n",
    "        elif module_def[\"type\"] == \"upsample\":\n",
    "            upsample = Upsample(scale_factor=int(module_def[\"stride\"]), mode=\"nearest\")\n",
    "            modules.add_module(f\"upsample_{module_i}\", upsample)\n",
    "\n",
    "        elif module_def[\"type\"] == \"route\":\n",
    "            layers = [int(x) for x in module_def[\"layers\"].split(\",\")]\n",
    "            filters = sum([output_filters[1:][i] for i in layers]) // int(module_def.get(\"groups\", 1))\n",
    "            modules.add_module(f\"route_{module_i}\", nn.Sequential())\n",
    "\n",
    "        elif module_def[\"type\"] == \"shortcut\":\n",
    "            filters = output_filters[1:][int(module_def[\"from\"])]\n",
    "            modules.add_module(f\"shortcut_{module_i}\", nn.Sequential())\n",
    "\n",
    "        elif module_def[\"type\"] == \"yolo\":\n",
    "            anchor_idxs = [int(x) for x in module_def[\"mask\"].split(\",\")] #[1 2 3]\n",
    "            # Extract anchors\n",
    "            anchors = [int(x) for x in module_def[\"anchors\"].split(\",\")] #[10 14  23 27  37 58  81 82  135 169  344 319]\n",
    "            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)] #[(10,14),(23,27),(37,58),(81,82),(135,169),(344,319)]\n",
    "            anchors = [anchors[i] for i in anchor_idxs] #[(23,27),(37,58),(81,82)]\n",
    "            num_classes = int(module_def[\"classes\"]) #20\n",
    "            # Define detection layer\n",
    "            yolo_layer = YOLOLayer(anchors, num_classes)\n",
    "            modules.add_module(f\"yolo_{module_i}\", yolo_layer)\n",
    "        # Register module list and number of output filters\n",
    "        module_list.append(modules)\n",
    "        output_filters.append(filters)\n",
    "\n",
    "    return hyperparams, module_list\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\" nn.Upsample is deprecated \"\"\"\n",
    "\n",
    "    def __init__(self, scale_factor, mode=\"nearest\"):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
    "        return x\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    \"\"\" The MISH activation function (https://github.com/digantamisra98/Mish) \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Mish, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(F.softplus(x))\n",
    "\n",
    "class YOLOLayer(nn.Module):\n",
    "    \"\"\"Detection layer\n",
    "    if training: return x[bs,3,13,13,25]\n",
    "           else: return x[bs,3*13*13,25]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, anchors, num_classes):\n",
    "        super(YOLOLayer, self).__init__()\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.num_classes = num_classes\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        self.no = num_classes + 5  # number of outputs per anchor\n",
    "        self.grid = torch.zeros(1)  # TODO\n",
    "\n",
    "        anchors = torch.tensor(list(chain(*anchors))).float().view(-1, 2)\n",
    "        self.register_buffer('anchors', anchors)\n",
    "        self.register_buffer(\n",
    "            'anchor_grid', anchors.clone().view(1, -1, 1, 1, 2))\n",
    "        self.stride = None\n",
    "\n",
    "    def forward(self, x, img_size):\n",
    "        stride = img_size // x.size(2) #416//13=32 input_img与featuremap的比例\n",
    "        self.stride = stride\n",
    "        bs, _, ny, nx = x.shape  # x(bs,75,13,13) to x(bs,3,13,13,25)\n",
    "        x = x.view(bs, self.num_anchors, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n",
    "\n",
    "        if not self.training:  # inference, 非训练时执行,生成回归参数用于画bbox,训练时只用求loss,在loss.py中生成\n",
    "            if self.grid.shape[2:4] != x.shape[2:4]:\n",
    "                self.grid = self._make_grid(nx, ny).to(x.device) #grid_cell左上角？ [1,1,13,13,2]\n",
    "\n",
    "            x[..., 0:2] = (x[..., 0:2].sigmoid() + self.grid) * stride  # featuremap上的xy放缩至原图\n",
    "            x[..., 2:4] = torch.exp(x[..., 2:4]) * self.anchor_grid # wa*exp(tw), ha*exp(th)\n",
    "            x[..., 4:] = x[..., 4:].sigmoid() #to\n",
    "            x = x.view(bs, -1, self.no) #[bs,3*13*13,25]\n",
    "\n",
    "        return x \n",
    "\n",
    "    @staticmethod\n",
    "    def _make_grid(nx=20, ny=20):\n",
    "        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])\n",
    "        return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()\n",
    "\n",
    "\n",
    "class Darknet(nn.Module):\n",
    "    \"\"\"YOLOv3 object detection model\"\"\"\n",
    "\n",
    "    def __init__(self, config_path):\n",
    "        super(Darknet, self).__init__()\n",
    "        self.module_defs = parse_model_config(config_path)\n",
    "        self.hyperparams, self.module_list = create_modules(self.module_defs)\n",
    "        self.yolo_layers = [layer[0]\n",
    "                            for layer in self.module_list if isinstance(layer[0], YOLOLayer)]\n",
    "        self.seen = 0\n",
    "        self.header_info = np.array([0, 0, 0, self.seen, 0], dtype=np.int32)\n",
    "        \n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "    def forward(self, x):   \n",
    "        \n",
    "        img_size = x.size(2)\n",
    "        layer_outputs, yolo_outputs = [], []\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
    "            if module_def[\"type\"] in [\"convolutional\"]:\n",
    "                # quantize  \n",
    "                x = self.quant(x)\n",
    "                x = module(x)  \n",
    "                # dequantize\n",
    "                x = self.dequant(x)\n",
    "                \n",
    "             \n",
    "            elif module_def[\"type\"] in [\"upsample\", \"maxpool\"]:\n",
    "                x = module(x)  \n",
    "            elif module_def[\"type\"] == \"route\":\n",
    "                combined_outputs = torch.cat([layer_outputs[int(layer_i)] for layer_i in module_def[\"layers\"].split(\",\")], 1)\n",
    "                group_size = combined_outputs.shape[1] // int(module_def.get(\"groups\", 1))\n",
    "                group_id = int(module_def.get(\"group_id\", 0))\n",
    "                x = combined_outputs[:, group_size * group_id : group_size * (group_id + 1)] # Slice groupings used by yolo v4\n",
    "            elif module_def[\"type\"] == \"shortcut\":\n",
    "                layer_i = int(module_def[\"from\"])\n",
    "                x = layer_outputs[-1] + layer_outputs[layer_i]\n",
    "            elif module_def[\"type\"] == \"yolo\":\n",
    "                x = module[0](x, img_size)  \n",
    "             \n",
    "                yolo_outputs.append(x)\n",
    "            layer_outputs.append(x)\n",
    "        #if training:[(bs,3,26,26,25),(bs,3,13,13,25)] else:[bs,3*26*26+3*13*13,25]    \n",
    "        return yolo_outputs if self.training else torch.cat(yolo_outputs, 1) \n",
    "    \n",
    "    def load_darknet_weights(self, weights_path):\n",
    "        \"\"\"Parses and loads the weights stored in 'weights_path'\"\"\"\n",
    "\n",
    "        # Open the weights file\n",
    "        with open(weights_path, \"rb\") as f:\n",
    "            # First five are header values\n",
    "            header = np.fromfile(f, dtype=np.int32, count=5)\n",
    "            self.header_info = header  # Needed to write header when saving weights\n",
    "            self.seen = header[3]  # number of images seen during training\n",
    "            weights = np.fromfile(f, dtype=np.float32)  # The rest are weights\n",
    "                     \n",
    "        #import pdb; pdb.set_trace()\n",
    "        # Establish cutoff for loading backbone weights\n",
    "        cutoff = None\n",
    "        # If the weights file has a cutoff, we can find out about it by looking at the filename\n",
    "        # examples: darknet53.conv.74 -> cutoff is 74\n",
    "        filename = os.path.basename(weights_path)\n",
    "        if \".conv.\" in filename:\n",
    "            try:\n",
    "                cutoff = int(filename.split(\".\")[-1])  # use last part of filename\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        ptr = 0\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
    "            if i == cutoff:\n",
    "                break\n",
    "            if module_def[\"type\"] == \"convolutional\" or module_def[\"type\"] == \"quantize_convolutional\":\n",
    "                conv_layer = module[0]\n",
    "                if module_def[\"batch_normalize\"]:\n",
    "                    # Load BN bias, weights, running mean and running variance\n",
    "                    bn_layer = module[1]\n",
    "                    num_b = bn_layer.bias.numel()  # Number of biases\n",
    "                    # Bias\n",
    "                    bn_b = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(bn_layer.bias)\n",
    "                    bn_layer.bias.data.copy_(bn_b)\n",
    "                    ptr += num_b\n",
    "                    # Weight\n",
    "                    bn_w = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(bn_layer.weight)\n",
    "                    bn_layer.weight.data.copy_(bn_w)\n",
    "                    ptr += num_b\n",
    "                    # Running Mean\n",
    "                    bn_rm = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(bn_layer.running_mean)\n",
    "                    bn_layer.running_mean.data.copy_(bn_rm)\n",
    "                    ptr += num_b\n",
    "                    # Running Var\n",
    "                    bn_rv = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(bn_layer.running_var)\n",
    "                    bn_layer.running_var.data.copy_(bn_rv)\n",
    "                    ptr += num_b\n",
    "                else:\n",
    "                    # Load conv. bias\n",
    "                    num_b = conv_layer.bias.numel()\n",
    "                    conv_b = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(conv_layer.bias)\n",
    "                    conv_layer.bias.data.copy_(conv_b)\n",
    "                    ptr += num_b\n",
    "                # Load conv. weights\n",
    "                num_w = conv_layer.weight.numel()\n",
    "                conv_w = torch.from_numpy(\n",
    "                    weights[ptr: ptr + num_w]).view_as(conv_layer.weight)\n",
    "                conv_layer.weight.data.copy_(conv_w)\n",
    "                ptr += num_w\n",
    "\n",
    "    def save_darknet_weights(self, path, cutoff=-1):\n",
    "        \"\"\"\n",
    "            @:param path    - path of the new weights file\n",
    "            @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)\n",
    "        \"\"\"\n",
    "        fp = open(path, \"wb\")\n",
    "        self.header_info[3] = self.seen\n",
    "        self.header_info.tofile(fp)\n",
    "\n",
    "        # Iterate through layers\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n",
    "            if module_def[\"type\"] == \"convolutional\" or module_def[\"type\"] == \"quantize_convolutional\":\n",
    "                conv_layer = module[0]\n",
    "                # If batch norm, load bn first\n",
    "                if module_def[\"batch_normalize\"]:\n",
    "                    bn_layer = module[1]\n",
    "                    bn_layer.bias.data.cpu().numpy().tofile(fp)\n",
    "                    bn_layer.weight.data.cpu().numpy().tofile(fp)\n",
    "                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)\n",
    "                    bn_layer.running_var.data.cpu().numpy().tofile(fp)\n",
    "                # Load conv bias\n",
    "                else:\n",
    "                    conv_layer.bias.data.cpu().numpy().tofile(fp)\n",
    "                # Load conv weights\n",
    "                conv_layer.weight.data.cpu().numpy().tofile(fp)\n",
    "\n",
    "        fp.close()\n",
    "\n",
    "class QuantizedDarknet(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(QuantizedDarknet, self).__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized.\n",
    "        # This will only be used for inputs.\n",
    "        self.quant = QuantStub()\n",
    "        # DeQuantStub converts tensors from quantized to floating point.\n",
    "        # This will only be used for outputs.\n",
    "        self.dequant = DeQuantStub()\n",
    "        # FP32 model\n",
    "        self.model_fp32 = model_fp32\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        #x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        #x = self.dequant(x)\n",
    "        return x                \n",
    "        \n",
    "def load_model(model_path, weights_path=None):\n",
    "    \"\"\"Loads the yolo model from file.\n",
    "\n",
    "    :param model_path: Path to model definition file (.cfg)\n",
    "    :type model_path: str\n",
    "    :param weights_path: Path to weights or checkpoint file (.weights or .pth)\n",
    "    :type weights_path: str\n",
    "    :return: Returns model\n",
    "    :rtype: Darknet\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "                          else \"cpu\")  # Select device for inference\n",
    "    model = Darknet(model_path).to(device) \n",
    "    \n",
    "    model.apply(weights_init_normal)\n",
    "   \n",
    "    # If pretrained weights are specified, start from checkpoint or weight file\n",
    "    if weights_path:\n",
    "        if weights_path.endswith(\".pth\"):\n",
    "            # Load checkpoint weights\n",
    "            model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "        else:\n",
    "            # Load darknet weights\n",
    "            model.load_darknet_weights(weights_path)\n",
    "            \n",
    "    #model_qt = QuantizedDarknet(model).to(device)     \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager Mode Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Darknet(\n",
       "  (module_list): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_0): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (maxpool_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (conv): Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_2): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (maxpool_3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (conv): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_4): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (maxpool_5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (conv): Conv2d(20, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_6): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (maxpool_7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (conv): Conv2d(40, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_8): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (maxpool_9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (conv): Conv2d(80, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_10): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (11): Sequential(\n",
       "      (_debug_padding_11): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      (maxpool_11): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (conv): Conv2d(160, 321, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(321, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_12): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (conv): Conv2d(321, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_13): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (conv): Conv2d(80, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_14): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (15): Sequential(\n",
       "      (conv): Conv2d(160, 27, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (16): Sequential(\n",
       "      (yolo_16): YOLOLayer(\n",
       "        (mse_loss): MSELoss()\n",
       "        (bce_loss): BCELoss()\n",
       "      )\n",
       "    )\n",
       "    (17): Sequential(\n",
       "      (route_17): Sequential()\n",
       "    )\n",
       "    (18): Sequential(\n",
       "      (conv): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_18): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (19): Sequential(\n",
       "      (upsample_19): Upsample()\n",
       "    )\n",
       "    (20): Sequential(\n",
       "      (route_20): Sequential()\n",
       "    )\n",
       "    (21): Sequential(\n",
       "      (conv): Conv2d(120, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_21): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (22): Sequential(\n",
       "      (conv): Conv2d(80, 27, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (23): Sequential(\n",
       "      (yolo_23): YOLOLayer(\n",
       "        (mse_loss): MSELoss()\n",
       "        (bce_loss): BCELoss()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "                          else \"cpu\")  # Select device for inference\n",
    "#model_fp32 = Darknet(\"../config/yolov3.cfg\").to(device)\n",
    "#model_fp32.load_state_dict(torch.load('../weights/yolov3.weights'), map_location=device)\n",
    "\n",
    "model_fp32 = load_model(\"../config/yolov3-tiny-prune.cfg\", './best_yolov3_prune.pth')\n",
    "torch.save(model_fp32.state_dict(),'./model_fp32.pth')\n",
    "\n",
    "model_to_quantize = copy.deepcopy(model_fp32)\n",
    "model_to_quantize.eval()\n",
    "# fuse\n",
    "# fused_list = nn.ModuleList()\n",
    "# for a in list(model_to_quantize.children())[0]:\n",
    "#     if isinstance(a, nn.Sequential):\n",
    "#         for i, b in enumerate(a):\n",
    "#             if isinstance(b, nn.modules.batchnorm.BatchNorm2d):\n",
    "#                 # fuse this bn layer with the previous conv2d layer\n",
    "#                 conv = a[i - 1]\n",
    "#                 fused = torch_utils.fuse_conv_and_bn(conv, b)\n",
    "#                 a = nn.Sequential(fused, *list(a.children())[i + 1:])\n",
    "#                 break\n",
    "#     fused_list.append(a)     \n",
    "# model_to_quantize.module_list = fused_list  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kit/tm/px6680/.local/lib/python3.8/site-packages/torch/quantization/observer.py:122: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_to_quantize.cpu()\n",
    "model_to_quantize.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "#model_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv', 'batch_norm']])\n",
    "model_fp32_prepared = torch.quantization.prepare(model_to_quantize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.datasets import ListDataset\n",
    "from utils.augmentations import AUGMENTATION_TRANSFORMS\n",
    "from utils.utils import to_cpu, load_classes, print_environment_info, provide_determinism, worker_seed_set\n",
    "\n",
    "def _create_data_loader(img_path, batch_size, img_size, n_cpu, multiscale_training=False):\n",
    "    dataset = ListDataset(\n",
    "        img_path,\n",
    "        img_size=img_size,\n",
    "        multiscale=multiscale_training,\n",
    "        transform=AUGMENTATION_TRANSFORMS)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=n_cpu,\n",
    "        pin_memory=True,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "        worker_init_fn=worker_seed_set)\n",
    "    return dataloader\n",
    "\n",
    "dataloader = _create_data_loader(\n",
    "        '../../../data/detrac/train.txt', #2012_train.txt中的路径\n",
    "        32,\n",
    "        416,\n",
    "        12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/87 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "Training:  93%|█████████▎| 81/87 [01:27<00:05,  1.06it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "Training: 100%|██████████| 87/87 [01:28<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "# in a real world setting, the calibration would be done with a representative dataset\n",
    "# 以下任选其一\n",
    "#for data in dataloader:\n",
    "#    model_fp32_prepared(data)\n",
    "# OR    \n",
    "for batch_i, (_, imgs, targets) in enumerate(tqdm.tqdm(dataloader, desc=f\"Training\")):\n",
    "    #imgs = imgs.to(device, non_blocking=True)\n",
    "    if batch_i % 20 == 0:\n",
    "        input_fp32 = imgs\n",
    "    #     model_fp32_prepared.to(device)\n",
    "        model_fp32_prepared(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kit/tm/px6680/.local/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    }
   ],
   "source": [
    "# convert without calibration\n",
    "#input_fp32 = torch.randn(16, 3, 416, 416)\n",
    "#model_fp32_prepared(input_fp32)\n",
    "#model_fp32_prepared.cpu()\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model_int8)\n",
    "torch.save(model_int8.state_dict(),'./yolov3-EagerModeQt.pth')\n",
    "#torch.save(model_fp32.state_dict(),'./checkpoints/yolov3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darknet(\n",
      "  (module_list): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (conv): QuantizedConv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.005828088615089655, zero_point=49, padding=(1, 1), bias=False)\n",
      "      (batch_norm): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leaky_0): QuantizedLeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (maxpool_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (conv): QuantizedConv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), scale=0.06230154260993004, zero_point=64, padding=(1, 1), bias=False)\n",
      "      (batch_norm): QuantizedBatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leaky_2): QuantizedLeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (maxpool_3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (conv): QuantizedConv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), scale=0.029878349974751472, zero_point=63, padding=(1, 1), bias=False)\n",
      "      (batch_norm): QuantizedBatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leaky_4): QuantizedLeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (maxpool_5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (conv): QuantizedConv2d(20, 40, kernel_size=(3, 3), stride=(1, 1), scale=0.03174806758761406, zero_point=69, padding=(1, 1), bias=False)\n",
      "      (batch_norm): QuantizedBatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leaky_6): QuantizedLeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (maxpool_7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (conv): QuantizedConv2d(40, 80, kernel_size=(3, 3), stride=(1, 1), scale=0.02695128321647644, zero_point=64, padding=(1, 1), bias=False)\n",
      "      (batch_norm): QuantizedBatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leaky_8): QuantizedLeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (maxpool_9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (conv): QuantizedConv2d(80, 160, kernel_size=(3, 3), stride=(1, 1), scale=0.01147056557238102, zero_point=64, padding=(1, 1), bias=False)\n",
      "      (batch_norm): QuantizedBatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leaky_10): QuantizedLeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (_debug_padding_11): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
      "      (maxpool_11): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (conv): QuantizedConv2d(160, 321, kernel_size=(3, 3), stride=(1, 1), scale=0.010348369367420673, zero_point=68, padding=(1, 1), bias=False)\n",
      "      (batch_norm): QuantizedBatchNorm2d(321, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leaky_12): QuantizedLeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (13): Sequential(\n",
      "      (conv): QuantizedConv2d(321, 80, kernel_size=(1, 1), stride=(1, 1), scale=0.0032777022570371628, zero_point=82, bias=False)\n",
      "      (batch_norm): QuantizedBatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leaky_13): QuantizedLeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (14): Sequential(\n",
      "      (conv): QuantizedConv2d(80, 160, kernel_size=(3, 3), stride=(1, 1), scale=0.003883308731019497, zero_point=40, padding=(1, 1), bias=False)\n",
      "      (batch_norm): QuantizedBatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leaky_14): QuantizedLeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (15): Sequential(\n",
      "      (conv): QuantizedConv2d(160, 27, kernel_size=(1, 1), stride=(1, 1), scale=0.18954551219940186, zero_point=66)\n",
      "    )\n",
      "    (16): Sequential(\n",
      "      (yolo_16): YOLOLayer(\n",
      "        (mse_loss): MSELoss()\n",
      "        (bce_loss): BCELoss()\n",
      "      )\n",
      "    )\n",
      "    (17): Sequential(\n",
      "      (route_17): Sequential()\n",
      "    )\n",
      "    (18): Sequential(\n",
      "      (conv): QuantizedConv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), scale=0.0019272017525509, zero_point=61, bias=False)\n",
      "      (batch_norm): QuantizedBatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leaky_18): QuantizedLeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (19): Sequential(\n",
      "      (upsample_19): Upsample()\n",
      "    )\n",
      "    (20): Sequential(\n",
      "      (route_20): Sequential()\n",
      "    )\n",
      "    (21): Sequential(\n",
      "      (conv): QuantizedConv2d(120, 80, kernel_size=(3, 3), stride=(1, 1), scale=0.008594762533903122, zero_point=55, padding=(1, 1), bias=False)\n",
      "      (batch_norm): QuantizedBatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leaky_21): QuantizedLeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (22): Sequential(\n",
      "      (conv): QuantizedConv2d(80, 27, kernel_size=(1, 1), stride=(1, 1), scale=0.17877429723739624, zero_point=66)\n",
      "    )\n",
      "    (23): Sequential(\n",
      "      (yolo_23): YOLOLayer(\n",
      "        (mse_loss): MSELoss()\n",
      "        (bce_loss): BCELoss()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (quant): Quantize(scale=tensor([0.0641]), zero_point=tensor([16]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cth2",
   "language": "python",
   "name": "cth2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
